{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2MgUqHwB-3Op"
      },
      "source": [
        "\n",
        "# **Exercise 10: Convolutional Neural Networks Part 2** AUTOENCODER"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wqa4tDJx_P0z"
      },
      "source": [
        "## **Exercise 09.1: Theory** "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ofpVdpCs_mDz"
      },
      "source": [
        "### **1.1 Describe the basic building blocks of a CNN in your own words**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "**1. Feature Extractor / Backbone:** Extracts (visual) features from the raw data necessary to solve the trained task. The feature extractor of a typical CNN consists of:\n",
        "\n",
        "**1.1 Convolutional Layers:** Convolution layers are layers where filters are applied to the original image or to other feature maps. The most important parameters of the layer are: \n",
        "\n",
        "**Number of Filters (out_channels):** The number of applied filters. This number corresponds to the number of feature_maps in the output block.\n",
        "\n",
        "**Stride:** Sets how far the filter moves in each step. For an image we can set the step size for the horizontal and vertical direction.\n",
        "\n",
        "**Kernel Size:** The size of a convolution filter (for image width and height)\n",
        "\n",
        "**Padding:** is used to control the height and width of the convolution layer output. For images, each side of the convolution layer input image is expanded with pixels filled with a predefined value.\n",
        "\n",
        "**1.2 Pooling Layers:**\n",
        "Pooling layers are used to reduce the size of feature maps by pooling the features in the feature map. The most important parameters are kernel size, stride and padding. There are several types of pooling layers:\n",
        "\n",
        "**Max Pooling:** Select the maximum value in the pooling kernel.\n",
        "\n",
        "**Average Pooling:** Calculate the average value of all values in the pooling kernel.\n",
        "\n",
        "**Global Max/Average Pooling:** Summarizes entire feature maps into a single layer by selecting the maximum value or calculating the average value.\n",
        "\n",
        "**2. Head:** The top of the network that uses the extracted features from the backbone and makes the final prediction for the given task. More complex models can have multiple heads that solve different tasks but share a backbone/feature extractor.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "dSOJVlruzxqD"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pwy8qjlY_9Pu"
      },
      "source": [
        "\n",
        "### **1.2 Describe two advantages of a CNN over an MLP for image-based classification**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "1. More parameter-efficient: A filter is slid over the image instead of transferring the entire image to a fully-conencted layer.\n",
        "2. It is easier for the CNN to use spatial information: The input image remains in its original form and is not converted into a flat, 1D vector.\n",
        "3. Better visual interpretability: The learned filters can be interpreted visually using various techniques. (Example of a online tools and articels: https://tensorspace.org/html/playground/lenet.html, https://distill.pub/2017/feature-visualization/, https://distill.pub/2018/building-blocks/)\n",
        "\n"
      ],
      "metadata": {
        "id": "enlgnMlb1CWm"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Cco8W1mAR9v"
      },
      "source": [
        "## **Exercise 09.2: Application**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EmFb5h-9Avzw"
      },
      "source": [
        "### **2.1 Read the Cifar10 or MNIST Dataset and plot some examples (train/eval/test split)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x0fsoqyz_Gl_"
      },
      "source": [
        "We use Pytorch to read the Cifar10 or MNIST dataset for our model and make sure that it is split into train/eval/test subsets. There is no official eval subset for Cifar10 (/ MNIST), so we use some data from the training subset to create our own eval subset. To see if the dataset is read correctly, we present some examples and the corresponding class with Matplotlib.\n",
        "\n",
        "Tip: torchvision.datasets is a simple, high-level API for downloading and reading datasets."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "DATASET = \"MNIST\" # MNIST or CIFAR"
      ],
      "metadata": {
        "id": "PZDlR0bWCZ-T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SZO49Tvm_KkX"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "from torchvision import datasets\n",
        "# some handy functions to use along widgets\n",
        "from IPython.display import display, Markdown, clear_output\n",
        "# widget packages\n",
        "import ipywidgets as widgets\n",
        "\n",
        "# Transforms allow us to apply transformations on the data. \n",
        "# A simple example is the transform \"transforms.ToTensor()\" which allows us to return a dataset of tensors instead of PIL images.\n",
        "# Other transformations include normalizations and augmentations, such as cropping parts of an image.\n",
        "from torchvision import transforms\n",
        "\n",
        "# For random processes in Torch, we can set an arbitrary number as the seed. This makes the program deterministic and the random processes behave exactly the same every time the program is started.\n",
        "torch.manual_seed(42)\n",
        "\n",
        "# All datasets are subclasses of torch.utils.data.Dataset and have a similar API. \n",
        "# With the first argument (root) we specify the location where we the data is located or saved.\n",
        "# If train=True the training set is loaded, if False the test set is loaded.\n",
        "# Many datasets can be downloaded automatically when download=True. \n",
        "# The transform argument allows us to apply transformations to the data. These transformations can consist of several transformations (especially augmentations) in real applications\n",
        "if DATASET == \"MNIST\":\n",
        "  dataset = datasets.MNIST('../data', train=True, download=True, transform=transforms.ToTensor())\n",
        "else:\n",
        "  dataset = datasets.CIFAR10('../data', train=True, download=True, transform=transforms.ToTensor())\n",
        "\n",
        "# Randomly split a dataset into non-overlapping new datasets of given lengths.\n",
        "# We use this function to create a training and evaluation set from the official training data.\n",
        "# The training set will have 40000 examples, the evaluation set 10000.\n",
        "if DATASET == \"MNIST\":\n",
        "  train_dataset, eval_dataset = torch.utils.data.random_split(dataset, [50000, 10000])\n",
        "else:\n",
        "  train_dataset, eval_dataset = torch.utils.data.random_split(dataset, [40000, 10000])\n",
        "\n",
        "# The test set has 10000 examples.\n",
        "if DATASET == \"MNIST\":\n",
        "  test_dataset = datasets.MNIST('../data', train=False, transform=transforms.ToTensor())\n",
        "else:\n",
        "  test_dataset = datasets.CIFAR10('../data', train=False, transform=transforms.ToTensor())\n",
        "\n",
        "# Print information of the dataset.\n",
        "print(\"Succesfully read the dataset:\")\n",
        "print(f\"The training data contains {len(train_dataset)} examples.\")\n",
        "print(f\"The evaluation data contains {len(eval_dataset)} examples.\")\n",
        "print(f\"The test data contains {len(test_dataset)} examples.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5v6LzzYEAnLs"
      },
      "source": [
        "Now we can use the dataset to plot some examples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R6K74f5B_XfZ"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# The label of each given sample (image) is simply a number. For visualization, we can create a map that maps this number to the class name.  \n",
        "cifar10_labels_map = {\n",
        "    0: \"airplane\",\n",
        "    1: \"automobile\",\n",
        "    2: \"bird\",\n",
        "    3: \"cat\",\n",
        "    4: \"deer\",\n",
        "    5: \"dog\",\n",
        "    6: \"frog\",\n",
        "    7: \"horse\",\n",
        "    8: \"ship\",\n",
        "    9: \"truck\",\n",
        "}\n",
        "\n",
        "# Prepare the plot.\n",
        "figure = plt.figure(figsize=(8, 8))\n",
        "\n",
        "# We plot 9 images in a 3x3 grid.\n",
        "cols, rows = 3, 3\n",
        "\n",
        "# For each iamge we want to plot:\n",
        "for i in range(1, cols * rows + 1):\n",
        "    # We create an index of the dataset at random to randomly select a sample.\n",
        "    sample_idx = torch.randint(len(train_dataset), size=(1,)).item()\n",
        "\n",
        "    # With this index we select the sample and its label.\n",
        "    img, label = train_dataset[sample_idx]\n",
        "\n",
        "    # And we plot the image with the corresponding label name.\n",
        "    figure.add_subplot(rows, cols, i)\n",
        "    if DATASET != \"MNIST\":\n",
        "      plt.title(cifar10_labels_map[label])\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "    # IMPORTANT: The sample is given in [c,w,h] shape([image-channels, image-widht, image-height] e.g., [3,32,32] for Cifar10). \n",
        "    # Pytorch uses this format for performance reasons on the hardware.\n",
        "    # To plot the image we need to bring it back into the \"normal\" [w,h,c] shape.\n",
        "    img = img.permute((1,2,0))\n",
        "    plt.imshow(img.squeeze())\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YwUd4xNPBTf9"
      },
      "source": [
        "### **2.2 Build your own CNN with Pytorch**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j0J70eE9BBiW"
      },
      "outputs": [],
      "source": [
        "from torch._C import NoneType\n",
        "import torch.nn as nn\n",
        "from torchsummary import summary\n",
        "\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# We define an CNN class that is a subclass of nn.Module.\n",
        "# nn.Module is the base class for all neural networks modules in Pytorch and provides basic functions \n",
        "class CNN(nn.Module):\n",
        "    def __init__(self, isMnist=False, preEmbeddingChannels=8):\n",
        "        super(CNN, self).__init__()\n",
        "\n",
        "        in_channels = 1 if isMnist else 3\n",
        "\n",
        "        # Convolutional Layer 1 with 32 filters and kernel 3\n",
        "        self.conv1 = nn.Conv2d(in_channels, 32, kernel_size=3,stride=2,padding=1)\n",
        "        # Convolutional Layer 2 with 64 filters and kernel 3\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3,stride=2,padding=1)\n",
        "        # Convolutional Layer 3 with 64 filters and kernel 3\n",
        "        self.conv3 = nn.Conv2d(64, preEmbeddingChannels, kernel_size=3,stride=2,padding=1)\n",
        "\n",
        "        self._size = None\n",
        "\n",
        "        # TODO: define the missing layers        \n",
        "\n",
        "\n",
        "    def encode(self,x):\n",
        "      x = F.relu(self.conv1(x))\n",
        "      x = F.relu(self.conv2(x))\n",
        "      x = F.relu(self.conv3(x))\n",
        "\n",
        "      if self._size == None:\n",
        "          self._size = x.shape\n",
        "        \n",
        "      e = torch.flatten(x, 1) # flatten all dimensions except batch\n",
        "      return e\n",
        "\n",
        "    def decode(self,e):\n",
        "        # this will give you the images with the same shape as after the last encoding conv layer\n",
        "        x = e.view(-1,self._size[1],self._size[2],self._size[3])\n",
        "\n",
        "        # TODO: perform the decoding using transpose convolutions and activations\n",
        "        return x\n",
        "\n",
        "        \n",
        "    def forward(self, x):\n",
        "        # encode the image\n",
        "        e = self.encode(x)\n",
        "\n",
        "        # decode back to image\n",
        "        x = self.decode(e)        \n",
        "        return x, e"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **2.2.2 Print the summary of the model**\n",
        "This helps us to iterativly define the missing layers until we are back to our target image shapes:\n",
        " - $1 \\times 28 \\times 28$ for MNIST\n",
        " - $3 \\times 32 \\times 32$ for CIFAR"
      ],
      "metadata": {
        "id": "c0OVWLtTUup_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "_model = CNN(isMnist=(DATASET==\"MNIST\")).cuda()\n",
        "\n",
        "if DATASET == \"MNIST\":\n",
        "  summary(_model, (1, 28, 28))\n",
        "else:\n",
        "  summary(_model, (3, 32, 32))"
      ],
      "metadata": {
        "id": "GnPTnYInVryz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JlDkepfmBqD0"
      },
      "source": [
        "### **2.3 Train your model on Cifar10. Use Tensorboard to log your runs.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vIi9KWusgE8v"
      },
      "outputs": [],
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "def train(train_loader, model, optimizer, epoch, log_interval, device):\n",
        "    # First, we put the model into training mode. \n",
        "    model.train()\n",
        "    \n",
        "    # We iterate over the entire data set. \n",
        "    # The variable data contains the images. \n",
        "    # The variable target contains the name of the images.\n",
        "    # batch_idx is the index of the current batch in the epoch.\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "\n",
        "        # We put the data onto the selected device.\n",
        "        # Shapes: \n",
        "        # data: [b,c,w,h]\n",
        "        # target: [b] - Single number of the label for each sample.\n",
        "        # autoencoder target = data\n",
        "        #data, target = data.to(device), target.to(device)\n",
        "        data, target = data.to(device), data.to(device)\n",
        "\n",
        "        # Sets the gradients of all optimized torch.Tensors to zero.\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # We call the forward function of our model to get the prediction of the model for our data.\n",
        "        # output shape: [b,10] -  The outputs of the 10 neurons in the last layer, where each neuron represents a class.\n",
        "        output, embedding = model(data)\n",
        "\n",
        "        # We calculate the training loss.\n",
        "        loss = F.mse_loss(output, target)\n",
        "\n",
        "        # Computes the gradient of current tensor w.r.t. graph leaves (input).\n",
        "        loss.backward()\n",
        "\n",
        "        # ll optimizers implement a step() method, that updates the parameters.\n",
        "        optimizer.step()\n",
        "\n",
        "        # We print the results each log_interval steps.\n",
        "        if batch_idx % log_interval == 0:\n",
        "          step = batch_idx * len(data)\n",
        "          epoch_progress = round((100. * batch_idx / len(train_loader)),2)\n",
        "          print(f\"Train Epoch: {epoch} [Step {step} from {len(train_loader.dataset)} ({epoch_progress}%)]\\t Loss: {loss.item()}\")\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BeLE3aTgBBwt"
      },
      "outputs": [],
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "# This function validates the model on the given data.\n",
        "def val(data_loader, model, training_epoch, device, mode=\"eval\"):\n",
        "    # First, we put the model into evaluation mode. \n",
        "    model.eval()\n",
        "    \n",
        "    # We define variables for the loss and the number of correctly classified samples.\n",
        "    loss = 0\n",
        "    correct = 0\n",
        "\n",
        "    # We do not want our validation function to participate in the gradient calculation (more on this in the next lecture).\n",
        "    with torch.no_grad():\n",
        "        # We iterate over the entire data set. \n",
        "        # The variable data contains the images. \n",
        "        # The variable target contains the name of the images.\n",
        "        for data, target in data_loader:\n",
        "\n",
        "            # We put the data onto the selected device.\n",
        "            # Shapes: \n",
        "            # data: [b,c,w,h]\n",
        "            # target: [b] - Single number of the label for each sample.\n",
        "            # autoencoder target = data\n",
        "            #data, target = data.to(device), target.to(device)\n",
        "            data, target = data.to(device), data.to(device)\n",
        "\n",
        "            # We call the forward function of our model to get the prediction of the model for our data.\n",
        "            # output shape: [b,10] -  The outputs of the 10 neurons in the last layer, where each neuron represents a class.\n",
        "            output, embedding = model(data)\n",
        "\n",
        "            # We calculate the validation loss.\n",
        "            loss += F.mse_loss(output, target, reduction='mean').item()  # sum up batch loss\n",
        "\n",
        "    # For total loss, the sum of the losses from each example is defined by the number of samples in the dataset.\n",
        "    loss /= len(data_loader)        \n",
        "\n",
        "    # We print the results.\n",
        "    print(f\"\\nValidation for {mode} data after Epoch: {training_epoch}\\t Loss {loss}\\n\")\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext tensorboard"
      ],
      "metadata": {
        "id": "mrlsncegaPKc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%tensorboard --logdir ./runs"
      ],
      "metadata": {
        "id": "ywu18H_nZ2Jp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ho1UhKo8gJFn"
      },
      "outputs": [],
      "source": [
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import torchvision.models as models\n",
        "\n",
        "# The writer to write date into tensorboard.\n",
        "writer = SummaryWriter()\n",
        "\n",
        "# Training Hyperparameters\n",
        "\n",
        "# Autoencoder: \n",
        "# Decide how many channels the resulting feature map could have AFTER encoding\n",
        "pre_embedding_channels = 4\n",
        "\n",
        "# We want to run our model on a GPU, so we choose \"cuda\" as the device. Alternatively, we can run our model on the CPU with \"cpu\".\n",
        "device = \"cuda\"\n",
        "# The batch size will be explained in more detail in the next lecture. With batch_size we define the number of samples that are fed to the model in one step.\n",
        "batch_size = 64\n",
        "\n",
        "# The number of epochs we want to train the model. \n",
        "epochs = 25\n",
        "\n",
        "# The learning rate used in the optimizer \n",
        "learning_rate = 0.0001\n",
        "\n",
        "# The number of classes in the dataset\n",
        "num_classes = 10 \n",
        "\n",
        "# Print results each log_interval_steps steps.\n",
        "log_interval_steps = 100\n",
        "\n",
        "# Test the model after each epoch.\n",
        "eval_interval_epochs = 1\n",
        "\n",
        "# With the DataLoader we can load data from our dataset step by step. \n",
        "# With the argument batch_size we can specify how mach samples are loaded in one batch.\n",
        "# With shuffel it can be specified whether the data set should be shuffled and returned in a random order. If all data is loaded once and shuffel=true, the data set will be shuffled again.\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "eval_loader = torch.utils.data.DataLoader(eval_dataset, batch_size=batch_size)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size)\n",
        "\n",
        "# We load the model into the VRAM of the GPU.\n",
        "model = CNN(isMnist=(DATASET==\"MNIST\"),preEmbeddingChannels=pre_embedding_channels).to(device)\n",
        "# model = models.resnet18(pretrained=False, num_classes=num_classes).to(device) \n",
        "# model = models.efficientnet_b0(pretrained=False, num_classes=num_classes).to(device) \n",
        "\n",
        "# To plot the model graph in tensorboard, we need an example input.\n",
        "example_image = train_dataset[0][0].unsqueeze(0).to(device)\n",
        "#writer.add_graph(model, example_image)\n",
        "\n",
        "# Print model\n",
        "print(model)\n",
        "\n",
        "# The optimizer used to train the model\n",
        "# With model.parameters() we inform the optimizer about the parameters taht shoudl be optimized.\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# For each epoch\n",
        "for epoch in range(1, epochs + 1):\n",
        "    # We train the model on the entire train data.\n",
        "    train_loss = train(train_loader, model, optimizer, epoch, log_interval_steps, device)\n",
        "\n",
        "    # Check if we want to evaluate the model at the end of this epoch\n",
        "    if epoch % eval_interval_epochs == 0:\n",
        "        # Evaluate the model on the entire train data.\n",
        "        eval_loss = val(eval_loader, model, epoch, device)\n",
        "        # Write the training loss, eval loss and eval accuracy into tensorboard.\n",
        "        writer.add_scalar('Loss/train', train_loss, epoch)\n",
        "        writer.add_scalar('Loss/eval', eval_loss, epoch)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# We test our final tuned model on the test data to report our final results.\n",
        "test_loss = val(eval_loader, model, epoch, device, mode=\"test\")\n",
        "# Write the training loss, test loss and test accuracy into tensorboard.\n",
        "writer.add_scalar('Loss/test', test_loss, epoch)"
      ],
      "metadata": {
        "id": "kw6kA3iy3CrC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **2.4 Visualize the results**"
      ],
      "metadata": {
        "id": "elaEpmOQVWlZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "def plotRandomSampleReconstruction():\n",
        "  sample_idx = torch.randint(len(test_dataset), size=(1,)).item()\n",
        "  img, _ = test_dataset[sample_idx]\n",
        "\n",
        "  input = torch.tensor(img).unsqueeze(0).to(device)\n",
        "\n",
        "  output, e = model(input)\n",
        "\n",
        "  input = img.permute((1,2,0))\n",
        "  output = output.clone().detach().cpu().numpy()\n",
        "  output = np.squeeze(output)\n",
        "\n",
        "  if DATASET != \"MNIST\":\n",
        "    output = np.moveaxis(output,0,-1)\n",
        "\n",
        "\n",
        "  fig, (ax1, ax2) = plt.subplots(1, 2)\n",
        "  ax1.imshow(np.squeeze(input))\n",
        "  ax2.imshow(np.squeeze(output))\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "4TCxXmUs5hy5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "button = widgets.Button(description='Plot Random Sample Reconstruction')\n",
        "out = widgets.Output()\n",
        "def on_button_clicked(_):\n",
        "      with out:\n",
        "          clear_output()\n",
        "          plotRandomSampleReconstruction()\n",
        "button.on_click(on_button_clicked)\n",
        "widgets.VBox([button,out])"
      ],
      "metadata": {
        "id": "6LcSHHt_SWGM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "t-1H8ENmSbNN"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "ofpVdpCs_mDz",
        "pwy8qjlY_9Pu"
      ],
      "name": "Exercise_10_Convolutional_Neural_Networks_Part_2.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}